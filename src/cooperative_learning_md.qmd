---
title: "Cooperative Learning"
author: "Leandro Kovalevski"
format:
  gfm:
    toc: true
    number-sections: true
    toc-depth: 2
    code-fold: true
    code-summary: "Show the code"
    highlight-style: github
theme:
  light: zephyr
  dark: cyborg
---



#  Executive summary {-}

- A database of a random sample of 23,857 tax identification numbers (CUITs) is analyzed. Past financial behavior is used to predict default. The percent of default in the dataset is 9.6%.
- Variable distributions and associations with the response are summarized.
- There is a clear (marginal) association between default and some variables ('col_3', 'col_2', 'col_6', 'col_8', 'col_17', 'col_20', 'col_21', 'col_22', 'col_2' and 'col_26') 
)
- The dataset was divided in training and testing sets in a 70-30 ratio.
- The Cooperative Learning model performance was compared with a Stepwise Logistic Regression model performance and a Random Forest model performance according to RMSE, AUC, and Lift 5%. Also a Accuracy, Recall, Precision and F1 Score were calculate for all models using the proportion of events in the training set as the probability threshold
- Different Cooperative Learning models were fitted varying the penalty parameter $\rho$. 
- The best performance of Cooperative Learning models was using a value of $\rho$ equal to 0.7, but Stepwise Logistic Regression outperformed all models on RMSE, AUC, and Lift 5%. 


#  Settings {-}

```{r}
#' Load data and needed packages.

#' Install (if needed)  'here' package to use relative paths. 
if(!("here" %in% installed.packages()[, "Package"])){ 
  install.packages("here") 
  }

#' Load generic functions ----
source(here::here("src", "utils.R"), encoding = "UTF-8")

#' Cargar las librer√≠as necesarias
loadPackages(c(
  # Data Preparation
  "here"
  , "dplyr"
  # Stats & Metrics
  , "pROC"
  , "Metrics"
  ,"gains"
  , "ROCR"
  , "skimr"
  , "scales"
  , "doBy"
  , "moments"
  # Machine Learning
  , "randomForest"
  , "multiview"
  # Visualization
  , "ggplot2"
  , "corrplot"
  , "knitr"
  , "broom"
  ))

#' Set data path
file_path <- here::here("data", "ready")

#' Set models path
models_path <- here::here("results", "models")



#' Set data file names
if( !exists("file_name") ){
  file_name <- "df_bcra.rds"
}

#' Data group (or views) name
group_file_name <- "groups.csv"

#' Read dataset
df <- readRDS(file.path(file_path, file_name))

#' Read dataset
groups <- read.csv2(file.path(file_path, group_file_name))

#' Define the response
response <- "response"

```



# 1. Dataset description {-}

The database consists of a random sample of 23,857 tax identification numbers (CUITs) belonging to individuals who had at least one debt in the Argentine financial system in June 2019, and were in credit situation 1 or 2 (meaning they did not have overdue payments exceeding 90 days), obtained from the debtor database provided by the Central Bank of the Argentine Republic (BCRA) for the period of June 2019.
For the tax identification numbers in the random sample, debts in all entities were recorded and summarized for June 2019, as well as for the previous 6 months. Debts of these tax identification numbers between July 2019 and June 2020 were also recorded to assess their evolution. 
The response variable is a binary variable constructed from the most severe credit situation of the tax identification number (CUIT) between the periods of July 2019 and June 2020. The variable takes the value 1 if the most severe credit situation is greater than or equal to 3 in any debt any period, and 0 otherwise.
In the dataset 'df_bcra.rds', the information recorded with 28 variables is available. The data is anonymized and variable names are not displayed.




# 2. Exploratory Data Analysis {-}

## 2.1. General descriptive analysis {-}

```{r}
skim(df)

```

## 2.2. Response descriptive analysis {-}

To analyze the default, we use the variable: **'response'**, which take the
following values: \n

- **0**: if the most severe credit situation is always less than 3 (always 
the payment delay is less than 90 days). \n
- **1**: if the most severe credit situation is greater than or equal to 3 
in any debt any period. \n



```{r}
#| output: asis

cat(paste0("\n### Response variabe: **", response, "**.\n"))

describeCategorical(
  data         = df,
  var          = response,
  bar_color    = "#fff159",
  sec          = "2.2"
  ) 


cat("\n")
cat("\n")



```





## 2.3. Descriptive analysis of categorical predictors {-}

```{r}
#| output: asis

#' Identify categorical and quantitavie variables
nvars <- names(df)[(sapply(X = df, FUN = class)) %in% 
                     c("integer", "numeric", "double") ]
cvars <- names(df)[(sapply(X = df, FUN = class)) %in% 
                     c("character", "factor", "logical", "text") ]

#' Delete unuseful variables
vars_to_exclude <- c( response, "id" )
nvars              <- nvars[!nvars %in% vars_to_exclude]



for (var in cvars){
  
  describeCategoricalAndBinary(
    data    = df, 
    var     = var, 
    binary  = response,
    ti      = which(cvars == var), 
    gi      = which(cvars == var),
    sec     = "2.3"
    )
}

```

## 2.4. Descriptive analysis of quantitative predictors {-}


```{r}
#| output: asis

for (var in nvars){

  describeNumericAndBinaryResponse(
    data   = df, 
    var    = var, 
    binary = response,
    ti     = which(nvars == var), 
    gi     = which(nvars == var),
    sec    = "2.4" 
    )
}

```


## 2.5. Matrix correlation of quantitative predictors {-}


```{r}
#| output: asis

M      <- cor(df[, nvars], use = "pairwise.complete.obs")
df_cor <- cor(df[, nvars])
corrplot(df_cor, order = 'hclust', addrect = 5)

```



# 3. Model Training {-}

## 3.1. Data preparation {-}

```{r}
# Convert character variables to factors and then remove unused factor levels
predictors_to_exclude <- c(response, "id", "col_18", "col_19")
cvars  <- cvars[! cvars %in% predictors_to_exclude]

df <- df %>%
  mutate(across(all_of(cvars), ~ if (is.character(.)) as.factor(.) else .)) %>%
  mutate(across(all_of(cvars), droplevels))

# Formula to create dummies
dummy_formula <- formula(paste0(" ~ ", paste(cvars, collapse = " + "), " - 1" ) )

# Convert categorical variables to dummy variables
dummy_vars <- model.matrix(dummy_formula, data = df)

# Split the data into a training set (70%) and a test set (30%)

# Set a seed to reproduce the results
set.seed(2000)

# Sample Indexes
indexes <- sample(1:nrow(df), size = round(0.3 * nrow(df)))

# Split data
df_ready    <- cbind(df[c(response, nvars)], dummy_vars)
df_to_train <- df_ready[-indexes,]
df_to_test  <- df_ready[ indexes,]

# Standardize numeric variables in the train set
numeric_vars_std <- scale(df_to_train[, nvars])

# Save means and standard deviations
df_standardization <- data.frame(
  variable = colnames(numeric_vars_std),
  mean     = attr(numeric_vars_std, "scaled:center"),
  sd       = attr(numeric_vars_std, "scaled:scale")
)
write.csv(df_standardization, file = file.path(file_path, "features_std.csv"),  row.names = FALSE)

# Standardize numeric variables in the test set
numeric_vars_test_std <- scale(df_to_test[, nvars], center = df_standardization$mean, scale = df_standardization$sd)

# Prepare train and test sets with standarized numeric variables
df_train <- cbind(df_to_train[, !colnames(df_to_train) %in% nvars], numeric_vars_std)
df_test  <- cbind(df_to_test[, !colnames(df_to_test) %in% nvars], numeric_vars_test_std)


# Predictors to exclude
predictors  <- colnames(df_train)[! colnames(df_train) %in% predictors_to_exclude]

# Model formula to analyze the renponse variable with all the predictors
model_formula <- formula(paste0(response, " ~ ", paste(predictors, collapse = " + ") ) )


performance_models  <- data.frame()
prop_df_train       <- prop.table(table(df_train[response]))[2]

```

## 3.2. Stepwise Logistic Regression {-}

A stepwise logistic regression is fitted.

```{r}
#| output: asis

#' Full model
rl_full <- glm(model_formula, family = binomial(link = 'logit'), data = df_train, control = glm.control(epsilon = 1e-8, maxit = 50))

#' Model with only the intercept
rl_intercepto <- glm(response ~ 1, family = binomial(link = 'logit'), data = df_train, control = glm.control(epsilon = 1e-8, maxit = 50))

fit_rl <- step(
  rl_intercepto, 
  scope = list(lower = rl_intercepto, upper = rl_full), 
  direction = "both",
  trace = 0
) 
cat(paste0("\n### Stepwise Logistic Regression Coefficients.\n"))
kable(tidy(fit_rl))


#' Logistic Regression Metrics
predictions <- predict(fit_rl, df_test, type = "response")

performance_rl <- calculate_metrics(
  name   = "Log Reg",
  y_real = as.numeric(as.character(df_test[[response]])), # it is necessary to convert the factor variable to numeric
  y_prob = predictions,
  cutoff = prop_df_train
)

cat(paste0("\n### Logistic Regression Performance Metrics\n"))
knitr::kable(performance_rl, digits = 3)

performance_models <- rbind(
  performance_models, 
  performance_rl
)


```

## 3.3. Random Forest {-}


```{r}

nt = 500
df_train[[response]] <- as.factor(df_train[[response]])

randomForest <- randomForest(
  model_formula,
  data       = df_train,  
  ntree      = nt,  
  mtry       = 5,    
  importance = TRUE
) 

#' Random Forest metrics
predictions <- predict(randomForest, df_test, type = "prob")[, 2]

performance_rf <- calculate_metrics(
  name   = paste0("Random Forest - ntree ", nt),
  y_real = as.numeric(as.character(df_test[[response]])), 
  y_prob = predictions,
  cutoff = prop_df_train
)

cat(paste0("\n"))
cat(paste0("\n### Random Forest Performance Metrics\n"))
cat(paste0("\n"))


knitr::kable(performance_rf, digits = 3)

performance_models <- rbind(
  performance_models, 
  performance_rf
)



```

## 3.4. Cooperative Learning Models {-}



```{r}
#' ### Split data in the views.
groups <- groups %>%
  mutate(prefix = paste0("g", group))
```



```{r}

# Create vector lists of variables that belong to each group 
for (group in unique(groups$prefix)) {
  # Vectors with the features of each group
  assign(group, groups$feature[groups$prefix == group])
  # Data set of each group
  index <- which(group == unique(groups$prefix))
  assign(paste0("x", index), as.matrix(df_train[, get(group)]))
}

for (group in unique(groups$prefix)) {
  # Data set of each group
  index <- which(group == unique(groups$prefix))
  assign(paste0("test_x", index), as.matrix(df_test[, get(group)]))
}


```

### 3.4.1 Cooperative Learning with penalty parameter ($\rho$) equal to 0.5



```{r}
#' Create a folder for the model
rho        <- 0.5
n_groups   <- length(unique(groups$prefix))
n_vars     <- sum(table(groups$prefix)) 
model_name <- paste("CoopLearn_", "r", rho, "_g", n_groups, "_p", n_vars, sep = "")

#' Create the folder, if it does not exist
dir.create(file.path(models_path, model_name), showWarnings = FALSE)

#' Define the response
y      <- df_train[, response]
y_test <- df_test[, response]


if (!file.exists(file.path(models_path, model_name, "predictions.csv"))) {
  
  # Fit model
  start_time <- Sys.time()
  fit_coop = cv.multiview(
    list(x1, x2, x3, x4, x5, x6), 
    y, 
    family       = binomial(), 
    type.measure = "deviance", 
    nfolds       = 3,
    rho          = rho,
    trace.it     = TRUE
  )
  
  finish_time <- Sys.time()
  coop_learning_time <- difftime(finish_time, start_time, units = "sec") 
  
  # Lasso penalty parameter
  reg <- fit_coop$lambda.min
  
  #' Calculate predictions
  pred_multiview <- predict(
    fit_coop, 
    newx = list(test_x1, test_x2, test_x3, test_x4, test_x5, test_x6), 
    s = reg, type = "response"
  )
  
  #' Calculate the metrics
  performance_cl01 <- calculate_metrics(
    name   = model_name,
    y_real = y_test, 
    y_prob = as.vector(pred_multiview),
    cutoff = prop_df_train
  )
  
  performance_models <- rbind(
    performance_models, 
    performance_cl01
  )
  
  #' Save the model object
  saveRDS(fit_coop, file = file.path(models_path, model_name, "model_object.rds"))
  #' Save execution time
  write.csv(coop_learning_time, file = file.path(models_path, model_name, "execution_time.csv"))
  #' Save predictions
  df_to_kpis <- data.frame(y_test, pred_multiview)
  write.csv(df_to_kpis, file = file.path(models_path, model_name, "predictions.csv"))
  #' Save kpis
  write.csv(performance_cl01, file = file.path(models_path, model_name, "kpis.csv"))
} else {
  # Read risk, metrics, and importance
  dfPredictions        <- read.csv(file = file.path(models_path, model_name, "predictions.csv")) %>% 
    select(-X)
  kpis                 <- read.csv(file = file.path(models_path, model_name, "kpis.csv"))[, -1]
  fit_coop             <- readRDS(file = file.path(models_path, model_name, "model_object.rds")) 
  coop_learning_time   <- read.csv(file = file.path(models_path, model_name, "execution_time.csv"))[, 2] 
  
  
  performance_models <- rbind(
    performance_models, 
    kpis
  )
  
}

#' #### Model: **`r paste0(model_name)`**.


#' #### Adjusted coefficients
cat(paste0("\n"))
cat(paste0("\n### Coefficients by view\n"))
cat(paste0("\n"))
coef(fit_coop, s = "lambda.min")

#' #### Kpis
cat(paste0("\n"))
cat(paste0("\n### Coop Learning Performance Metrics - $$\rho$$ = 0.5\n"))
cat(paste0("\n"))
cat(paste0("\n"))
cat(paste0("\n### Coop Learning Performance Metrics - $$\\rho$$ = 0.5\n"))
cat(paste0("\n"))
cat(paste0("\n"))
cat(paste0("\n### Coop Learning Performance Metrics - $\\rho$ = 0.5\n"))
cat(paste0("\n"))

knitr::kable(kpis, digits = 3)




#'
#'
#' ## CoopLearning - Model 1 rho = 0.7
#' 
#' 

#' Create a folder for the model
rho        <- 0.7
n_groups   <- length(unique(groups$prefix))
n_vars     <- sum(table(groups$prefix)) 
model_name <- paste("CoopLearn_", "r", rho, "_g", n_groups, "_p", n_vars, sep = "")

#' Create the folder, if it does not exist
dir.create(file.path(models_path, model_name), showWarnings = FALSE)

#' Define the response
y      <- df_train[, response]
y_test <- df_test[, response]


if (!file.exists(file.path(models_path, model_name, "predictions.csv"))) {
  
  # Fit model
  start_time <- Sys.time()
  fit_coop = cv.multiview(
    list(x1, x2, x3, x4, x5, x6), 
    y, 
    family       = binomial(), 
    type.measure = "deviance", 
    nfolds       = 3,
    rho          = rho,
    trace.it     = TRUE
  )
  
  finish_time <- Sys.time()
  coop_learning_time <- difftime(finish_time, start_time, units = "sec") 
  
  # Lasso penalty parameter
  reg <- fit_coop$lambda.min
  
  #' Calculate predictions
  pred_multiview <- predict(
    fit_coop, 
    newx = list(test_x1, test_x2, test_x3, test_x4, test_x5, test_x6), 
    s = reg, type = "response"
  )
  
  #' Calculate the metrics
  performance_cl01 <- calculate_metrics(
    name   = model_name,
    y_real = y_test, 
    y_prob = as.vector(pred_multiview),
    cutoff = prop_df_train
  )
  
  performance_models <- rbind(
    performance_models, 
    performance_cl01
  )
  
  #' Save the model object
  saveRDS(fit_coop, file = file.path(models_path, model_name, "model_object.rds"))
  #' Save execution time
  write.csv(coop_learning_time, file = file.path(models_path, model_name, "execution_time.csv"))
  #' Save predictions
  df_to_kpis <- data.frame(y_test, pred_multiview)
  write.csv(df_to_kpis, file = file.path(models_path, model_name, "predictions.csv"))
  #' Save kpis
  write.csv(performance_cl01, file = file.path(models_path, model_name, "kpis.csv"))
} else {
  # Read risk, metrics, and importance
  dfPredictions        <- read.csv(file = file.path(models_path, model_name, "predictions.csv")) %>% 
    select(-X)
  kpis                 <- read.csv(file = file.path(models_path, model_name, "kpis.csv"))[, -1]
  fit_coop             <- readRDS(file = file.path(models_path, model_name, "model_object.rds")) 
  coop_learning_time   <- read.csv(file = file.path(models_path, model_name, "execution_time.csv"))[, 2] 
  
  
  performance_models <- rbind(
    performance_models, 
    kpis
  )
}



#' #### Adjusted coefficients
coef(fit_coop, s = "lambda.min")

#' #### Kpis
cat(paste0("\n"))
cat(paste0("\n### Coop Learning Performance Metrics II\n"))
cat(paste0("\n"))
knitr::kable(kpis, digits = 3)


#'
#'
#' ## CoopLearning - Model 1 rho = 1
#' 
#' 

#' Create a folder for the model
rho        <- 1
n_groups   <- length(unique(groups$prefix))
n_vars     <- sum(table(groups$prefix)) 
model_name <- paste("CoopLearn_", "r", rho, "_g", n_groups, "_p", n_vars, sep = "")

#' Create the folder, if it does not exist
dir.create(file.path(models_path, model_name), showWarnings = FALSE)

#' Define the response
y      <- df_train[, response]
y_test <- df_test[, response]


if (!file.exists(file.path(models_path, model_name, "predictions.csv"))) {
  
  # Fit model
  start_time <- Sys.time()
  fit_coop = cv.multiview(
    list(x1, x2, x3, x4, x5, x6), 
    y, 
    family       = binomial(), 
    type.measure = "deviance", 
    nfolds       = 3,
    rho          = rho,
    trace.it     = TRUE
  )
  
  finish_time <- Sys.time()
  coop_learning_time <- difftime(finish_time, start_time, units = "sec") 
  
  # Lasso penalty parameter
  reg <- fit_coop$lambda.min
  
  #' Calculate predictions
  pred_multiview <- predict(
    fit_coop, 
    newx = list(test_x1, test_x2, test_x3, test_x4, test_x5, test_x6), 
    s = reg, type = "response"
  )
  
  #' Calculate the metrics
  performance_cl01 <- calculate_metrics(
    name   = model_name,
    y_real = y_test, 
    y_prob = as.vector(pred_multiview),
    cutoff = prop_df_train
  )
  
  performance_models <- rbind(
    performance_models, 
    performance_cl01
  )
  
  #' Save the model object
  saveRDS(fit_coop, file = file.path(models_path, model_name, "model_object.rds"))
  #' Save execution time
  write.csv(coop_learning_time, file = file.path(models_path, model_name, "execution_time.csv"))
  #' Save predictions
  df_to_kpis <- data.frame(y_test, pred_multiview)
  write.csv(df_to_kpis, file = file.path(models_path, model_name, "predictions.csv"))
  #' Save kpis
  write.csv(performance_cl01, file = file.path(models_path, model_name, "kpis.csv"))
} else {
  # Read risk, metrics, and importance
  dfPredictions        <- read.csv(file = file.path(models_path, model_name, "predictions.csv")) %>% 
    select(-X)
  kpis                 <- read.csv(file = file.path(models_path, model_name, "kpis.csv"))[, -1]
  fit_coop             <- readRDS(file = file.path(models_path, model_name, "model_object.rds")) 
  coop_learning_time   <- read.csv(file = file.path(models_path, model_name, "execution_time.csv"))[, 2] 
  
  
performance_models <- rbind(
  performance_models, 
  kpis
)
}

#' #### Model: **`r paste0(model_name)`**.


#' #### Adjusted coefficients
coef(fit_coop, s = "lambda.min")

#' #### Kpis
cat(paste0("\n### Coop Learning Performance Metrics III\n"))
knitr::kable(kpis, digits = 3)



#'
#'
#' ## CoopLearning - Model 1 rho = 2
#' 
#' 

#' Create a folder for the model
rho        <- 2
n_groups   <- length(unique(groups$prefix))
n_vars     <- sum(table(groups$prefix)) 
model_name <- paste("CoopLearn_", "r", rho, "_g", n_groups, "_p", n_vars, sep = "")

#' Create the folder, if it does not exist
dir.create(file.path(models_path, model_name), showWarnings = FALSE)

#' Define the response
y      <- df_train[, response]
y_test <- df_test[, response]


if (!file.exists(file.path(models_path, model_name, "predictions.csv"))) {
  
  # Fit model
  start_time <- Sys.time()
  fit_coop = cv.multiview(
    list(x1, x2, x3, x4, x5, x6), 
    y, 
    family       = binomial(), 
    type.measure = "deviance", 
    nfolds       = 3,
    rho          = rho,
    trace.it     = TRUE
  )
  
  finish_time <- Sys.time()
  coop_learning_time <- difftime(finish_time, start_time, units = "sec") 
  
  # Lasso penalty parameter
  reg <- fit_coop$lambda.min
  
  #' Calculate predictions
  pred_multiview <- predict(
    fit_coop, 
    newx = list(test_x1, test_x2, test_x3, test_x4, test_x5, test_x6), 
    s = reg, type = "response"
  )
  
  #' Calculate the metrics
  performance_cl01 <- calculate_metrics(
    name   = model_name,
    y_real = y_test, 
    y_prob = as.vector(pred_multiview),
    cutoff = prop_df_train
  )
  
  performance_models <- rbind(
    performance_models, 
    performance_cl01
  )
  
  #' Save the model object
  saveRDS(fit_coop, file = file.path(models_path, model_name, "model_object.rds"))
  #' Save execution time
  write.csv(coop_learning_time, file = file.path(models_path, model_name, "execution_time.csv"))
  #' Save predictions
  df_to_kpis <- data.frame(y_test, pred_multiview)
  write.csv(df_to_kpis, file = file.path(models_path, model_name, "predictions.csv"))
  #' Save kpis
  write.csv(performance_cl01, file = file.path(models_path, model_name, "kpis.csv"))
} else {
  # Read risk, metrics, and importance
  dfPredictions        <- read.csv(file = file.path(models_path, model_name, "predictions.csv")) %>% 
    select(-X)
  kpis                 <- read.csv(file = file.path(models_path, model_name, "kpis.csv"))[, -1]
  fit_coop             <- readRDS(file = file.path(models_path, model_name, "model_object.rds")) 
  coop_learning_time   <- read.csv(file = file.path(models_path, model_name, "execution_time.csv"))[, 2] 
  
  performance_models <- rbind(
    performance_models, 
    kpis
  )
}

#' #### Adjusted coefficients
coef(fit_coop, s = "lambda.min")

#' #### Kpis
cat(paste0("\n### Coop Learning Performance Metrics IV\n"))
knitr::kable(kpis, digits = 3)


```



# 4. Models Comparison {-}



```{r}

knitr::kable(performance_models, digits = 3)


```