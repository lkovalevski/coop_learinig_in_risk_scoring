---
title: "Cooperative Learning"
author: "Leandro Kovalevski"
format:
  gfm:
    toc: true
    number-sections: true
    toc-depth: 2
    code-fold: true
    code-summary: "Show the code"
    highlight-style: github
theme:
  light: zephyr
  dark: cyborg
---



#  Executive summary {-}

- A database of a random sample of 23,857 tax identification numbers (CUITs) is analyzed. Past financial behavior is used to predict default. The percent of default in the dataset is 9.6%.
- Variable distributions and associations with the response are presented.
- There is a clear (marginal) association between default and some variables ('col_3', 'col_2', 'col_6', 'col_8', 'col_17', 'col_20', 'col_21', 'col_22', 'col_2' and 'col_26') 
)
- The dataset was divided in training and testing sets in a 70-30 ratio.
- The Cooperative Learning model performance was compared with a Logistic Regression model performance and a Random Forest model performance.
- ...
- ...
- to be completed..
- ... 
- ...


#  Settings {-}

```{r}
#' Load data and needed packages.

#' Install (if needed)  'here' package to use relative paths. 
if(!("here" %in% installed.packages()[, "Package"])){ 
  install.packages("here") 
  }

#' Load generic functions ----
source(here::here("src", "utils.R"), encoding = "UTF-8")

#' Cargar las librer√≠as necesarias
loadPackages(c(
  "here"
  , "multiview", "scales", "dplyr", "doBy", "moments",
  "gains", "ROCR", "skimr", "moments", "corrplot"
  # Stats & Metrics
  , "pROC"
  , "Metrics"
  # Machine Learning
  , "randomForest"
  # Visualization
  , "ggplot2"
  , "knitr"
  , "broom"
  ))

#' Set data path
file_path <- here::here("data", "ready")



#' Set data file names
if( !exists("file_name") ){
  file_name <- "df_bcra.rds"
}

#' read data
df <- readRDS(file.path(file_path, file_name))

```



# 1. Dataset description {-}

The database consists of a random sample of 23,857 tax identification numbers (CUITs) belonging to individuals who had at least one debt in the Argentine financial system in June 2019, and were in credit situation 1 or 2 (meaning they did not have overdue payments exceeding 90 days), obtained from the debtor database provided by the Central Bank of the Argentine Republic (BCRA) for the period of June 2019.
For the tax identification numbers in the random sample, debts in all entities were recorded and summarized for June 2019, as well as for the previous 6 months. Debts of these tax identification numbers between July 2019 and June 2020 were also recorded to assess their evolution. 
The response variable is a binary variable constructed from the most severe credit situation of the tax identification number (CUIT) between the periods of July 2019 and June 2020. The variable takes the value 1 if the most severe credit situation is greater than or equal to 3 in any debt any period, and 0 otherwise.
In the dataset 'df_bcra.rds', the information recorded with 28 variables is available. The data is anonymized and variable names are not displayed.




# 2. Exploratory Data Analysis {-}

## 2.1. General descriptive analysis {-}

```{r}
skim(df)

```

## 2.2. Response descriptive analysis {-}

To analyze the default, we use the variable: **'response'**, which take the
following values: \n

- **0**: if the most severe credit situation is always less than 3 (always 
the payment delay is less than 90 days). \n
- **1**: if the most severe credit situation is greater than or equal to 3 
in any debt any period. \n



```{r}
#| output: asis

response <- "response"

cat(paste0("\n### Response variabe: **", response, "**.\n"))

describeCategorical(
  data         = df,
  var          = response,
  bar_color    = "#fff159",
  sec          = "2.2"
  ) 


cat("\n")
cat("\n")



```





## 2.3. Descriptive analysis of categorical predictors {-}

```{r}
#| output: asis

#' Identify categorical and quantitavie variables
nvars <- names(df)[(sapply(X = df, FUN = class)) %in% 
                     c("integer", "numeric", "double") ]
cvars <- names(df)[(sapply(X = df, FUN = class)) %in% 
                     c("character", "factor", "logical", "text") ]

for (var in cvars){
  
  describeCategoricalAndBinary(
    data    = df, 
    var     = var, 
    binary  = response,
    ti      = which(cvars == var), 
    gi      = which(cvars == var),
    sec     = "2.3"
    )
}

```

## 2.4. Descriptive analysis of quantitative predictors {-}


```{r}
#| output: asis

vars_to_exclude <- c( response, "id" )
nvars              <- nvars[!nvars %in% vars_to_exclude]


for (var in nvars){

  describeNumericAndBinaryResponse(
    data   = df, 
    var    = var, 
    binary = response,
    ti     = which(nvars == var), 
    gi     = which(nvars == var),
    sec    = "2.4" 
    )
}

```


## 2.5. Matrix correlation of quantitative predictors {-}


```{r}
#| output: asis

M      <- cor(df[, nvars], use = "pairwise.complete.obs")
df_cor <- cor(df[, nvars])
corrplot(df_cor, order = 'hclust', addrect = 5)

```



# 3. Model Training {-}

## 3.1. Training and testing sets {-}

```{r}
# Split the data into a training set (70%) and a test set (30%)

# Set a seed to reproduce the results
set.seed(2000)

# Sample Indexes
indexes = sample(1:nrow(df), size = round(0.3 * nrow(df)))

# Split data
df_train = df[-indexes, ]
df_test  = df[ indexes, ]

# Predictors to exclude
predictors_to_exclude <- c(response, "id", "col_18", "col_19")
predictors            <- colnames(df)[! colnames(df) %in% predictors_to_exclude]

# Model formula to analyze the renponse variable with all the predictors
model_formula <- formula(paste0(response, " ~ ", paste(predictors, collapse = " + ") ) )

# Evaluation Metrics
#' We create a function to calculate validation metrics

calculate_metrics <- function(name = "model", y_real, y_prob, cutoff = 0.5 ) {
  # Root Mean Squared Error (RMSE)
  rmse_value <- rmse(y_real, y_prob)
  
  # Area Under the Curve (AUC)
  roc_obj   <- roc(response = y_real, predictor = y_prob)
  auc_value <- roc_obj$auc
  
  # Lift for the top 10% of probabilities
  n_top_5       <- ceiling(0.05 * length(y_prob))
  top_5_indices <- order(y_prob, decreasing = TRUE)[1:n_top_5]
  lift_value    <- mean(y_real[top_5_indices]) / mean(y_real)
  
  # Binarized predictions according to the cutoff point
  y_pred <- ifelse(y_prob >= cutoff, 1, 0)
  
  # Confusion matrix
  confusion_matrix <- table(y_real, y_pred)
  
  # Calculation of metrics
  accuracy    <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  recall      <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
  precision   <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
  specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[1, ])
  f1_score    <- 2 * ((precision * recall) / (precision + recall))
  
  # Create a data frame with the metrics
  metrics <- data.frame(
    Model       = name,
    RMSE        = rmse_value,
    AUC         = auc_value,
    Lift_5      = lift_value,
    Accuracy    = accuracy,
    Recall      = recall,
    Precision   = precision,
    Specificity = specificity,
    F1_Score    = f1_score
  )
  
  return(metrics)
}


performance_models  <- data.frame()
prop_df_train       <- prop.table(table(df_train[response]))[2]

```

## 3.2. Training models {-}

```{r}
#| output: asis

cat(paste0("\n### Logistic Regression.\n"))
cat(paste0("\n A stepped wise logistic regression is fitted.\n"))

#' Full model
rl_full <- glm(model_formula, family = binomial(link = 'logit'), data = df_train)

kable(tidy(rl_full))


#' Model with only the intercept
rl_intercepto <- glm(response ~ 1, family = binomial(link = 'logit'), data = df_train)

fit_rl <- step(
  rl_intercepto, 
  scope = list(lower = rl_intercepto, upper = rl_full), 
  direction = "both", # direction can be "both", "forward", "backward"
  trace = 0
) 
kable(tidy(fit_rl))


#' Logistic Regression Metrics
predictions <- predict(fit_rl, df_test, type = "response")

performance_rl <- calculate_metrics(
  name   = "Log Reg",
  y_real = as.numeric(as.character(df_test[[response]])), # it is necessary to convert the factor variable to numeric
  y_prob = predictions,
  cutoff = prop_df_train
)

performance_models <- rbind(
  performance_models, 
  performance_rl
)

knitr::kable(performance_models, digits = 3)


cat(paste0("\n### Random Forest.\n"))


#'  
#' ## Random Forest
#' 

nt = 300
df_train[[response]] <- as.factor(df_train[[response]])

randomForest <- randomForest(
  model_formula,
  data       = df_train,  
  ntree      = nt,  
  mtry       = 5,    
  importance = TRUE
) 

#' Random Forest metrics
predictions <- predict(randomForest, df_test, type = "prob")[, 2]

performance_rf <- calculate_metrics(
  name   = paste0("Random Forest - ntree ", nt),
  y_real = as.numeric(as.character(df_test[[response]])), 
  y_prob = predictions,
  cutoff = prop_df_train
)

performance_models <- rbind(
  performance_models, 
  performance_rf
)

knitr::kable(performance_models, digits = 3)



cat(paste0("\n### Cooperative Learning.\n"))

```
